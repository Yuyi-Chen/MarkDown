## 爬虫是什么

爬虫，是近几年较为火热的一个词，在整日与数据为伴的互联网时代，人们每天都与各式各样的数据打交道，浏览器展示的网页、你的聊天记录、电商展现的产品描述等，都是数据。如果将这些数据加以分析，会得到一些有价值的信息，举个简单的例子，你打算在某宝买一件商品，这件商品可能有 N 家店铺都在出售，那你可能要比较不同商家的价格及好评度，从而决定去哪一家购买，在这个过程中你已经进行了数据的分析。进行数据分析的第一步就是采集数据，在上面的例子中，你通过打开不同商家的产品页面，得到了商品的价格和好评度等，这就是数据的采集过程。

上面的例子只是简单的数据的采集和分析，但是对于一个企业来说，需要采集的数据动辄都会达到成千上万条，如果通过人力去采集这些数据，那成本是极其高的，而爬虫，就是为之而生。网络爬虫（又称为网页蜘蛛，网络机器人），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。通过爬虫，我们可以轻松的得到互联网上的各种信息。

## 爬虫并非肆无忌惮，不要做法外狂徒张三

既然爬虫是用来爬取网络中的数据的，那么是任何数据都可以去爬吗？当然是不可以的，虽然爬虫近几年才开始发展，有关法律部分还在逐步建立与完善的过程中，但这并不意味着爬虫不受法律的制约。

首先，国际互联网界通过自身的协议建立了一个道德规范，即 Robots 协议，全称为“网络爬虫排除标准”，网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。该协议是国际互联网界通行的道德规范，虽然没有写入法律，但是每一个爬虫都应该遵守这项协议。

该协议一般放在网站的根目录下，只需在 url 后加 robots.text，如掘金的首页网址为 https://juejin.cn/，所以掘金网站 Robots 协议的地址为 https://juejin.cn/robots.txt ，如图**（Disallow 下的链接是不允许爬虫访问的）**：

![截屏2021-09-16 上午10.15.01](/Users/administrator/Desktop/截屏2021-09-16 上午10.15.01.png)

其次，由于高频率的网络爬虫会对服务器产生巨大的压力，网站可能封锁你的 IP，甚至采取进一步的法律行动。

所以，在使用爬虫时，我们应该约束自己的行为，以免产生不良的影响和后果。

## 万事具备，开启爬虫之旅

### 工欲善其事，必先利其器

我们选择使用 Python 作为开发语言，当前 Python 的最新版本为 3.9.7，由于我使用的是 Mac 电脑，所以本文围绕 Mac OS 系统展开。

#### Python 的安装

在 Mac OS 中有自带的 Python，我的电脑自带的 Python 版本为 2.7.16，可以通过 `python --version `命令查看电脑的 Python 版本。

![截屏2021-09-16 下午1.45.48](/Users/administrator/Desktop/截屏2021-09-16 下午1.45.48.png)

当然如果你想用最新的版本，只需要去 Python 的官方网站 https://www.python.org/  进行下载安装即可。

![截屏2021-09-16 下午1.53.55](/Users/administrator/Desktop/截屏2021-09-16 下午1.53.55.png)

打开下载后的文件然后一直点击下一步即可。

![截屏2021-09-16 下午1.57.30](/Users/administrator/Desktop/截屏2021-09-16 下午1.57.30.png)

安装完之后我们使用 `python --version` 查看版本，发现还是原来的版本，我们将命令换成 `python3 --version` 后，即可查看刚刚安装的 Python 版本。

![截屏2021-09-16 下午2.01.51](/Users/administrator/Desktop/截屏2021-09-16 下午2.01.51.png)

#### Pycharm 的安装与激活

我们选择 Pycharm 作为我们开发的 IDE，打开[ Pycharm 官方网站](https://www.jetbrains.com/zh-cn/pycharm/)点击下载即可，下载安装完成后，我们发现是需要激活的，具体的激活方式可以点击这个链接查看：https://shimo.im/docs/DJ3h3tJv98ppTYyH/read

### 开启爬虫之路

完成上述准备工作之后我们就可以进行爬虫的编码工作了，一般来说，网络爬虫需要三个流程：

1. 获取网页。
2. 解析网页（提取数据）。
3. 存储数据。

#### 获取网页

获取网页我们需要使用  Requests 库，如果你使用的是 Python2，可以通过 `pip install requests` 命令进行安装，如果使用的是 Python3，则使用 `pip3 install requests` 进行安装。

现在我们使用 Requests 获取网页的内容，我们以百度首页为例。

```python
import requests

r = requests.get('https://www.baidu.com')
print("文本编码：", r.encoding)
print("响应状态码：", r.status_code)
print("字符串方式的响应体:", r.text)
```

r 为 response 响应对象，它存储了服务器的响应内容，我们可以从中获取需要的信息，上述代码的执行结果如图：

![截屏2021-09-16 下午3.01.35](/Users/administrator/Library/Application Support/typora-user-images/截屏2021-09-16 下午3.01.35.png)

对 response 对象的属性介绍如下：

> 1. r.text是服务器响应的内容，会自动根据响应头部的字符编码进行解码。
> 2. r.encoding是服务器内容使用的文本编码。
> 3. r.status_code用于检测响应的状态码，如果返回200，就表示请求成功了；如果返回的是4xx，就表示客户端错误；返回5xx则表示服务器错误响应。我们可以用r.status_code来检测请求是否正确响应。
> 4. r.content是字节方式的响应体，会自动解码gzip和defate编码的响应数据。
> 5. r.json()是Requests中内置的JSON解码器。

上面访问的 url 是不带参数的，有的 url 还会带有参数，通常用‘ ? ’进行连接，用‘ & ’分隔每个参数，如我们在百度搜索矿中输入内容进行搜索，其 url 就是带参数的，如：`https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&rsv_idx=1&ch=&tn=baidu&bar=&wd=%E7%88%AC%E8%99%AB&rn=&fenlei=256&oq=&rsv_pq=ea3c630d0000a791&rsv_t=47aaynmqziOVoJJfw5w250Bd9uaaAF9vn6uj6oXTGT6aFidaTsLvLOUz918&rqlang=cn&rsv_enter=1&rsv_btype=i&rsv_dl=ib&inputT=1371`

在 Requests 中我们可以将参数直接存在字典中，用 params 构建到 url 中，如上述 url，我们可以通过下面的代码进行访问。

```python
import requests

k = {"wd": "pachong"}
r = requests.get('https://www.baidu.com/s', params=k)
print("拼接后的url", r.url)
```

通过上述代码的输出结果我们可以看到 url 已经被拼接完成：`拼接后的url https://www.baidu.com/s?wd=pachong`

在爬虫中，还有一个属性极其重要，那就是请求头，如果没有请求头或者请求头与实际网页不一致，就可能导致无法返回正确的结果，那如何获取到正确的请求头呢，我们只需要在浏览器中单击右键点击检查，在 network 中找到访问的地址，即可查看到请求头，如图：

![截屏2021-09-16 下午3.37.39](/Users/administrator/Library/Application Support/typora-user-images/截屏2021-09-16 下午3.37.39.png)

我们提取请求头中的重要部分，可以将代码改为：

```python
import requests

headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0'
                  '.4577.82 Safari/537.36',
    'Host': 'www.baidu.com'
}
r = requests.get('https://www.baidu.com', headers=headers)
print("响应状态码", r.status_code)
```

除了 get 方法，我们还经常通过 post 方法进行网络请求，如登录时，如果我们使用 get 请求，密码就会暴露在 url 中，很不安全。

跟 get 请求带参数类似，如果我们要通过 post 请求发送数据，我们只需要传递一个字典给 Requests 的 data 参数，这个数据字典就会在发出请求的时候自动编码为表单形式。

当我们进行爬虫时，如果服务器没有响应请求，程序就会一直等待，所以我们需要设置 Requests 的 timeout 参数，当服务器超过 timeout 设定的时间还没有返回时，就会返回异常。

#### 解析网页

通过上面的学习，我们已经可以获取到网页的内容，但是我们可以看到，返回的数据是一大串的字符串，所以我们要解析返回的数据，一般解析网页数据有三种方法，分别是正则表达式、BeautifulSoup 和 lxml。

| 方法          | 性能 | 易用性 | 提取数据方式        |
| ------------- | ---- | ------ | ------------------- |
| 正则表达式    | 快   | 较难   | 正则表达式          |
| BeautifulSoup | 快   | 简单   | find方法，css选择器 |
| lxml          | 快   | 简单   | XPath，css选择器    |

上表为三种方法的对比，本文我们选择使用 lxml 进行网页数据的解析。

使用 `pip3 install lxml` 下载 lxml。

我们先通过 Requests 获取一下“豆瓣读书TOP250”来作为我们解析的网页。

```python
headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.'
                  '4577.82 Safari/537.36'
}
r = requests.get('https://book.douban.com/top250', headers=headers)
print(r.text)
```

lxml 主要通过 XPAth 语法进行解析，如下表：

| 表达式            | 描述                                       |
| ----------------- | ------------------------------------------ |
| nodename          | 提取此节点的所有子节点                     |
| /                 | 从当前节点选取直接子节点                   |
| //                | 从当前节点选取子孙节点                     |
| .                 | 选取当前节点                               |
| ..                | 选取当前节点的父节点                       |
| @                 | 选择属性                                   |
| *                 | 通配符，选择所有元素节点与元素名           |
| @*                | 选取所有属性                               |
| [@attrib]         | 选取具有给定属性的所有元素                 |
| [@attrib='value'] | 选取给定属性具有给定值的所有元素           |
| [tag]             | 选取所有具有指定元素的直接子节点           |
| [tag='text']      | 选取所有具有指定元素并且文本内容是text节点 |

下面我们通过解析“豆瓣图书TOP250”来进一步的理解一下 Xpath 的各种表达式。

- 读取文本解析节点

```python
from lxml import etree

# r 为前面请求到的response
html = etree.HTML(r.text)
result=etree.tostring(html,encoding='utf-8')   #解析对象输出代码
print(type(html))
print(type(result))
print(result.decode('utf-8'))

out:
<class 'lxml.etree._Element'>
<class 'bytes'>
<html lang="zh-cmn-Hans" class="ua-mac ua-webkit book-new-nav">
<head>
...
    <!-- dae-web-book--default-7f85485954-fz8bw-->
</body>
</html>
```

- 获取所有节点

返回一个列表，列表的每个元素都是 Element 类型，所有节点都包含在其中。

```python
result = html.xpath('//*') # //代表获取子孙节点，*代表所有
print(type(html))
print(type(result))
print(result)

out:
<class 'lxml.etree._Element'>
<class 'list'>
[<Element html at 0x7ffec08b7780>, <Element head at 0x7ffec098a100>, <Element meta at 0x7ffec098a380>, <Element title at 0x7ffec098a3c0>, ... , <Element script at 0x7ffec09c9100>]

```

- 获取子节点

通过 `/` 或者` //` 即可查找元素的子节点或者子孙节点，如果想选择 head 节点的所有直接 title 节点，可以这样使用

```python
result = html.xpath('//head/title')
```

- 获取父节点

查找父节点可以使用 `..` 来实现也可以使用 `parent::` 来获取父节点，如

```python
result=html.xpath('//a[@href="link2.html"]/../@class')
result1=html.xpath('//a[@href="link2.html"]/parent::*/@class')
```

- 属性匹配

在选取的时候，我们还可以用`@`符号进行属性过滤。比如，这里如果要选取`class`为 `item` 的`tr`节点，可以这样实现

```python
result=html.xpath('//tr[@class="item"]')
```

- 文本获取

XPath中使用 `text()` 方法获取节点中的文本

```python
result = html.xpath('//p[@class="pl"]/text()')
for re in result:
    print(re)
    
out:
[清] 曹雪芹 著 / 人民文学出版社 / 1996-12 / 59.70元
余华 / 作家出版社 / 2012-8-1 / 20.00元
[哥伦比亚] 加西亚·马尔克斯 / 范晔 / 南海出版公司 / 2011-6 / 39.50元
...
[英] 毛姆 / 傅惟慈 / 上海译文出版社 / 2006-8 / 15.00元
```

- 属性多值匹配

如果某个属性的值有多个时，我们可以使用 `contains()` 函数来获取

```python
result1 = html.xpath('//p[@class="p"]/text()')
print(result1)
result2 = html.xpath('//p[contains(@class, "p")]/text()')
print(result2)

out:
# 通过第一种方法没有取到值，通过contains（）就能精确匹配到节点了
[]
['豆瓣', '[清] 曹雪芹 著 / 人民文学出版社 / 1996-12 / 59.70元', '余华 / 作家出版社 / 2012-8-1 / 20.00元', '[哥伦比亚] 加西亚·马尔克斯 / 范晔 / 南海出版公司 / 2011-6 / 39.50元', '[英] 乔治·奥威尔 / 刘绍铭 / 北京十月文艺出版社 / 2010-4-1 / 28.00', '[美国] 玛格丽特·米切尔 / 李美华 / 译林出版社 / 2000-9 / 40.00元', '刘慈欣 / 重庆出版社 / 2012-1-1 / 168.00元', '[明] 罗贯中 / 人民文学出版社 / 1998-05 / 39.50元', '[日] 东野圭吾 / 刘姿君 / 南海出版公司 / 2013-1-1 / 39.50元', '[法] 圣埃克苏佩里 / 马振聘 / 人民文学出版社 / 2003-8 / 22.00元', '[英] 阿·柯南道尔 / 丁钟华 等 / 群众出版社 / 1981-8 / 53.00元/68.00元', '林奕含 / 北京联合出版公司 / 2018-2 / 45.00元', '[英] 乔治·奥威尔 / 荣如德 / 上海译文出版社 / 2007-3 / 10.00元', '三毛 / 哈尔滨出版社 / 2003-8 / 15.80元', '金庸 / 生活·读书·新知三联书店 / 1994-5 / 96.00元', '（丹麦）安徒生 / 叶君健 / 人民文学出版社 / 1997-08 / 25.00元', '路遥 / 人民文学出版社 / 2005-1 / 64.00元', '钱锺书 / 人民文学出版社 / 1991-2 / 19.00', '[哥伦比亚] 加西亚·马尔克斯 / 杨玲 / 南海出版公司 / 2012-9-1 / 39.50元', '[法] 阿尔贝·加缪 / 柳鸣九 / 上海译文出版社 / 2010-8 / 22.00元', '当年明月 / 中国海关出版社 / 2009-4 / 358.20元', '王小波 / 中国青年出版社 / 1997-10 / 27.00元', '[以色列] 尤瓦尔·赫拉利 / 林俊宏 / 中信出版社 / 2014-11 / 68.00元', 'J.K.罗琳 (J.K.Rowling) / 苏农 / 人民文学出版社 / 2008-12-1 / 498.00元', '[美] 卡勒德·胡赛尼 / 李继宏 / 上海人民出版社 / 2006-5 / 29.00元', '[英] 毛姆 / 傅惟慈 / 上海译文出版社 / 2006-8 / 15.00元']

```

- 多属性匹配

如果根据多个属性确定一个节点，这时就需要同时匹配多个属性，此时可用运用 `and` 运算符来连接使用：

```python
result = html.xpath('//span[@id="icp" and @class="fleft gray-link"]/text()')
print(result)

out:
['\n    © 2005－2021 douban.com, all rights reserved 北京豆网科技有限公司\n']
```

#### 存储数据

当我们从网页中爬取到数据之后，我们一般会将数据存储起来，如存到文件或存储到数据库，这里我们将书的出版信息存储到 txt 文件中。

```python
result = html.xpath('//p[@class="pl"]/text()')
with open('book.txt', 'w') as f:
    f.write(json.dumps(result))
    f.close()
```

## 永无止境

通过上面的学习我们已经可以爬取一个网页的内容，解析我们想要的数据并存储到文件中了，看起来是不是很简单，那么爬虫就真的这么简单吗？

并不是，上面所说的只是爬虫的基本流程，在实际操作中我们还会遇到许多问题，如如何提升爬虫的速度、如何破解反爬虫、如何处理登录和验证码等。所以，还有更多的知识等着我们去探索和学习，期待下次再见👋